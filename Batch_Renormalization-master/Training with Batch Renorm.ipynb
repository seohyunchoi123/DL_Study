{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from Util import batch_renorm, batch_norm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_wd = 'C:\\\\Users\\\\CSH\\\\Desktop\\\\image_example\\\\train\\\\'\n",
    "test_wd = 'C:\\\\Users\\\\CSH\\\\Desktop\\\\image_example\\\\test\\\\'\n",
    "resizing=(32,32)\n",
    "\n",
    "train_x = np.empty([1,32,32,3])\n",
    "train_y = np.empty(1)\n",
    "idx=0\n",
    "for path, dir, files in os.walk(train_wd):\n",
    "    if path == train_wd:\n",
    "        ch_to_idx = {idx:t for idx,t in enumerate(dir)}\n",
    "        continue\n",
    "    for file in files[:50]:\n",
    "        image_dir = path + '/' + file \n",
    "        img = Image.open(image_dir)\n",
    "        img = img.resize(resizing)\n",
    "        if not img.format == \"RGB\": # 이미지의 포맷이 RGB가 아닐 경우, RGB로 convert 시킴\n",
    "            img = img.convert(\"RGB\")\n",
    "        train_x = np.vstack([train_x, np.array(img).reshape([1,32,32,3])])\n",
    "        train_y = np.vstack([train_y, idx])\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.empty([1,32,32,3])\n",
    "test_y = np.empty(1)\n",
    "idx=0\n",
    "for path, dir, files in os.walk(test_wd):\n",
    "    if path == test_wd:\n",
    "        ch_to_idx = {idx:t for idx,t in enumerate(dir)}\n",
    "        continue\n",
    "    for file in files:\n",
    "        image_dir = path + '/' + file \n",
    "        img = Image.open(image_dir)\n",
    "        img = img.resize(resizing)\n",
    "        if not img.format == \"RGB\": # 이미지의 포맷이 RGB가 아닐 경우, RGB로 convert 시킴\n",
    "            img = img.convert(\"RGB\")\n",
    "        test_x = np.vstack([test_x, np.array(img).reshape([1,32,32,3])])\n",
    "        test_y = np.vstack([test_y, idx])\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x[1:]\n",
    "train_y = train_y[1:]\n",
    "test_x = test_x[1:]\n",
    "test_y = test_y[1:]\n",
    "\n",
    "train_y = train_y.astype(int)\n",
    "train_y = train_y.reshape(-1)\n",
    "test_y = test_y.astype(int)\n",
    "test_y = test_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 32, 32, 3)\n",
      "(200,)\n",
      "(84, 32, 32, 3)\n",
      "(84,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_x))\n",
    "print(np.shape(train_y))\n",
    "print(np.shape(test_x))\n",
    "print(np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bicycle', 1: 'horse', 2: 'ship', 3: 'truck'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Without BatchNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph_1 = tf.Graph()\n",
    "with train_graph_1.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "    y = tf.placeholder(tf.int32, shape=[None])\n",
    "    y_onehot = tf.one_hot(y, depth=4)\n",
    "    \n",
    "    w = tf.get_variable(shape = [3,3,3,32], name='w')\n",
    "    w_softmax = tf.get_variable(shape = [16*16*32,4], name='w_softmax')\n",
    "    b_softmax = tf.get_variable(shape = [4], name = 'b_softmax')\n",
    "    \n",
    "    layer = tf.nn.conv2d(filter=w, input=x, padding='SAME', strides=[1,1,1,1])\n",
    "    # batch normal x\n",
    "    layer = tf.nn.relu(layer)\n",
    "    layer = tf.nn.max_pool(layer, ksize=[1,2,2,1], padding='SAME', strides=[1,2,2,1])\n",
    "    \n",
    "    logit = tf.matmul(tf.reshape(layer, shape=[-1,16*16*32]), w_softmax) + b_softmax\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_onehot))\n",
    "    training = tf.train.GradientDescentOptimizer(0.000001).minimize(loss) # gradientoptimizer로하니까 학습 거의안됨 .. lr을 엄청나게 낮춰야되는구나 \n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.argmax(logit, 1),dtype=tf.float32), tf.cast(y,dtype=tf.float32)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_epoch=15\n",
    "acc_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------- Epoch : 0 -----------------------------------------------------\n",
      "Loss :  119.52539\n",
      "Train Accuracy :  0.855\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 1 -----------------------------------------------------\n",
      "Loss :  38.826393\n",
      "Train Accuracy :  0.95\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 2 -----------------------------------------------------\n",
      "Loss :  18.437176\n",
      "Train Accuracy :  0.975\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 3 -----------------------------------------------------\n",
      "Loss :  11.084818\n",
      "Train Accuracy :  0.99\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 4 -----------------------------------------------------\n",
      "Loss :  7.7499285\n",
      "Train Accuracy :  0.995\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 5 -----------------------------------------------------\n",
      "Loss :  5.848634\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 6 -----------------------------------------------------\n",
      "Loss :  4.6043425\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 7 -----------------------------------------------------\n",
      "Loss :  3.746591\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.61904764\n",
      "------------------------------------------- Epoch : 8 -----------------------------------------------------\n",
      "Loss :  3.1508265\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.61904764\n",
      "------------------------------------------- Epoch : 9 -----------------------------------------------------\n",
      "Loss :  2.711862\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.61904764\n",
      "------------------------------------------- Epoch : 10 -----------------------------------------------------\n",
      "Loss :  2.371064\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 11 -----------------------------------------------------\n",
      "Loss :  2.0933506\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 12 -----------------------------------------------------\n",
      "Loss :  1.8553891\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 13 -----------------------------------------------------\n",
      "Loss :  1.6606495\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.64285713\n",
      "------------------------------------------- Epoch : 14 -----------------------------------------------------\n",
      "Loss :  1.4982084\n",
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.64285713\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph_1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch) :\n",
    "        batch_start=0\n",
    "        total_idx = np.arange(len(train_x)) ; #np.random.shuffle(total_idx)\n",
    "        for _ in range(int(np.ceil(len(train_x)/batch_size))):\n",
    "            batch_idx = total_idx[batch_start:batch_start+batch_size]\n",
    "            batch_x = train_x[total_idx]\n",
    "            batch_y = train_y[total_idx]\n",
    "            sess.run(training, feed_dict={x:batch_x,\n",
    "                                          y:batch_y})\n",
    "            batch_start += batch_size\n",
    "        print('------------------------------------------- Epoch : {} -----------------------------------------------------' .format(epoch))\n",
    "        print('Loss : ', sess.run(loss, feed_dict={x:batch_x,\n",
    "                                                   y:batch_y}))\n",
    "        print('Train Accuracy : ', sess.run(acc, feed_dict={x:batch_x,\n",
    "                                                          y:batch_y}))\n",
    "        accuracy = sess.run(acc, feed_dict={x:test_x,\n",
    "                                              y:test_y})\n",
    "        print('Test Accuracy : ', accuracy)\n",
    "        acc_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN With BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph_2 = tf.Graph()\n",
    "with train_graph_2.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "    y = tf.placeholder(tf.int32, shape=[None])\n",
    "    y_onehot = tf.one_hot(y, depth=4)\n",
    "    phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "    w = tf.get_variable(shape = [3,3,3,32], name='w')\n",
    "    w_softmax = tf.get_variable(shape = [16*16*32,4], name='w_softmax')\n",
    "    b_softmax = tf.get_variable(shape = [4], name = 'b_softmax')\n",
    "\n",
    "    layer = tf.nn.conv2d(filter=w, input=x, padding='SAME', strides=[1,1,1,1])\n",
    "    layer = batch_norm(layer, n_out=32, phase_train = phase_train)\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "    layer = tf.nn.max_pool(layer, ksize=[1,2,2,1], padding='SAME', strides=[1,2,2,1])\n",
    "\n",
    "    logit = tf.matmul(tf.reshape(layer, shape=[-1,16*16*32]), w_softmax) + b_softmax\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_onehot))\n",
    "    training = tf.train.GradientDescentOptimizer(0.000001).minimize(loss) # gradientoptimizer로하니까 학습 거의안됨 .. lr을 엄청나게 낮춰야되는구나 \n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.argmax(logit, 1),dtype=tf.float32), tf.cast(y,dtype=tf.float32)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_epoch=15\n",
    "acc_list_2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------- Epoch : 0 -----------------------------------------------------\n",
      "Loss :  220.12291\n",
      "Train Accuracy :  0.51\n",
      "Test Accuracy :  0.5714286\n",
      "------------------------------------------- Epoch : 1 -----------------------------------------------------\n",
      "Loss :  182.89407\n",
      "Train Accuracy :  0.645\n",
      "Test Accuracy :  0.60714287\n",
      "------------------------------------------- Epoch : 2 -----------------------------------------------------\n",
      "Loss :  158.04329\n",
      "Train Accuracy :  0.7\n",
      "Test Accuracy :  0.63095236\n",
      "------------------------------------------- Epoch : 3 -----------------------------------------------------\n",
      "Loss :  139.70285\n",
      "Train Accuracy :  0.76\n",
      "Test Accuracy :  0.6785714\n",
      "------------------------------------------- Epoch : 4 -----------------------------------------------------\n",
      "Loss :  125.375\n",
      "Train Accuracy :  0.785\n",
      "Test Accuracy :  0.6904762\n",
      "------------------------------------------- Epoch : 5 -----------------------------------------------------\n",
      "Loss :  113.72629\n",
      "Train Accuracy :  0.815\n",
      "Test Accuracy :  0.71428573\n",
      "------------------------------------------- Epoch : 6 -----------------------------------------------------\n",
      "Loss :  103.98638\n",
      "Train Accuracy :  0.87\n",
      "Test Accuracy :  0.71428573\n",
      "------------------------------------------- Epoch : 7 -----------------------------------------------------\n",
      "Loss :  95.64653\n",
      "Train Accuracy :  0.885\n",
      "Test Accuracy :  0.72619045\n",
      "------------------------------------------- Epoch : 8 -----------------------------------------------------\n",
      "Loss :  88.45264\n",
      "Train Accuracy :  0.885\n",
      "Test Accuracy :  0.72619045\n",
      "------------------------------------------- Epoch : 9 -----------------------------------------------------\n",
      "Loss :  82.163895\n",
      "Train Accuracy :  0.91\n",
      "Test Accuracy :  0.7380952\n",
      "------------------------------------------- Epoch : 10 -----------------------------------------------------\n",
      "Loss :  76.61052\n",
      "Train Accuracy :  0.91\n",
      "Test Accuracy :  0.7619048\n",
      "------------------------------------------- Epoch : 11 -----------------------------------------------------\n",
      "Loss :  71.66945\n",
      "Train Accuracy :  0.93\n",
      "Test Accuracy :  0.77380955\n",
      "------------------------------------------- Epoch : 12 -----------------------------------------------------\n",
      "Loss :  67.23722\n",
      "Train Accuracy :  0.94\n",
      "Test Accuracy :  0.78571427\n",
      "------------------------------------------- Epoch : 13 -----------------------------------------------------\n",
      "Loss :  63.22482\n",
      "Train Accuracy :  0.94\n",
      "Test Accuracy :  0.78571427\n",
      "------------------------------------------- Epoch : 14 -----------------------------------------------------\n",
      "Loss :  59.576546\n",
      "Train Accuracy :  0.945\n",
      "Test Accuracy :  0.78571427\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph_2) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch) :\n",
    "        batch_start=0\n",
    "        total_idx = np.arange(len(train_x)) ;# np.random.shuffle(total_idx)\n",
    "        for _ in range(int(np.ceil(len(train_x)/batch_size))):\n",
    "            batch_idx = total_idx[batch_start:batch_start+batch_size]\n",
    "            batch_x = train_x[total_idx]\n",
    "            batch_y = train_y[total_idx]\n",
    "            sess.run(training, feed_dict={x:batch_x,\n",
    "                                          y:batch_y,\n",
    "                                         phase_train:True})\n",
    "            batch_start += batch_size\n",
    "        print('------------------------------------------- Epoch : {} -----------------------------------------------------' .format(epoch))\n",
    "        print('Loss : ', sess.run(loss, feed_dict={x:batch_x,\n",
    "                                                   y:batch_y,\n",
    "                                                phase_train:False}))\n",
    "        print('Train Accuracy : ', sess.run(acc, feed_dict={x:batch_x,\n",
    "                                                            y:batch_y,\n",
    "                                                            phase_train:False}))\n",
    "        accuracy = sess.run(acc, feed_dict={x:test_x,\n",
    "                                            y:test_y,\n",
    "                                            phase_train:False})\n",
    "        print('Test Accuracy : ', accuracy)\n",
    "        acc_list_2.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN With BatchRenorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph_3 = tf.Graph()\n",
    "with train_graph_3.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "    y = tf.placeholder(tf.int32, shape=[None])\n",
    "    y_onehot = tf.one_hot(y, depth=4)\n",
    "    phase_train = tf.placeholder(tf.bool)\n",
    "    r_max = tf.placeholder(tf.float32, shape=[]) # scalar는 shape을 이렇게 비워야 함\n",
    "    d_max = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    w = tf.get_variable(shape = [3,3,3,32], name='w')\n",
    "    w_softmax = tf.get_variable(shape = [16*16*32,4], name='w_softmax')\n",
    "    b_softmax = tf.get_variable(shape = [4], name = 'b_softmax')\n",
    "\n",
    "    layer = tf.nn.conv2d(filter=w, input=x, padding='SAME', strides=[1,1,1,1])\n",
    "    layer = batch_norm(layer, n_out=32, phase_train = phase_train)\n",
    "    layer = batch_renorm(layer, n_out=32, phase_train = phase_train, r_max=r_max, d_max=d_max)\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "    layer = tf.nn.max_pool(layer, ksize=[1,2,2,1], padding='SAME', strides=[1,2,2,1])\n",
    "\n",
    "    logit = tf.matmul(tf.reshape(layer, shape=[-1,16*16*32]), w_softmax) + b_softmax\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_onehot))\n",
    "    training = tf.train.GradientDescentOptimizer(0.000001).minimize(loss) # gradientoptimizer로하니까 학습 거의안됨 .. lr을 엄청나게 낮춰야되는구나 \n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.argmax(logit, 1),dtype=tf.float32), tf.cast(y,dtype=tf.float32)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_epoch=15\n",
    "acc_list_3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------- Epoch : 0 -----------------------------------------------------\n",
      "Loss :  196.177\n",
      "Train Accuracy :  0.605\n",
      "Test Accuracy :  0.53571427\n",
      "------------------------------------------- Epoch : 1 -----------------------------------------------------\n",
      "Loss :  161.82492\n",
      "Train Accuracy :  0.7\n",
      "Test Accuracy :  0.61904764\n",
      "------------------------------------------- Epoch : 2 -----------------------------------------------------\n",
      "Loss :  139.3371\n",
      "Train Accuracy :  0.76\n",
      "Test Accuracy :  0.6785714\n",
      "------------------------------------------- Epoch : 3 -----------------------------------------------------\n",
      "Loss :  122.76203\n",
      "Train Accuracy :  0.82\n",
      "Test Accuracy :  0.6904762\n",
      "------------------------------------------- Epoch : 4 -----------------------------------------------------\n",
      "Loss :  109.919174\n",
      "Train Accuracy :  0.85\n",
      "Test Accuracy :  0.75\n",
      "------------------------------------------- Epoch : 5 -----------------------------------------------------\n",
      "Loss :  99.573616\n",
      "Train Accuracy :  0.865\n",
      "Test Accuracy :  0.7619048\n",
      "------------------------------------------- Epoch : 6 -----------------------------------------------------\n",
      "Loss :  90.994705\n",
      "Train Accuracy :  0.88\n",
      "Test Accuracy :  0.7619048\n",
      "------------------------------------------- Epoch : 7 -----------------------------------------------------\n",
      "Loss :  83.7076\n",
      "Train Accuracy :  0.895\n",
      "Test Accuracy :  0.7619048\n",
      "------------------------------------------- Epoch : 8 -----------------------------------------------------\n",
      "Loss :  77.43407\n",
      "Train Accuracy :  0.91\n",
      "Test Accuracy :  0.7619048\n",
      "------------------------------------------- Epoch : 9 -----------------------------------------------------\n",
      "Loss :  71.97019\n",
      "Train Accuracy :  0.915\n",
      "Test Accuracy :  0.77380955\n",
      "------------------------------------------- Epoch : 10 -----------------------------------------------------\n",
      "Loss :  67.14409\n",
      "Train Accuracy :  0.935\n",
      "Test Accuracy :  0.77380955\n",
      "------------------------------------------- Epoch : 11 -----------------------------------------------------\n",
      "Loss :  62.85022\n",
      "Train Accuracy :  0.94\n",
      "Test Accuracy :  0.79761904\n",
      "------------------------------------------- Epoch : 12 -----------------------------------------------------\n",
      "Loss :  59.018993\n",
      "Train Accuracy :  0.94\n",
      "Test Accuracy :  0.79761904\n",
      "------------------------------------------- Epoch : 13 -----------------------------------------------------\n",
      "Loss :  55.553204\n",
      "Train Accuracy :  0.96\n",
      "Test Accuracy :  0.79761904\n",
      "------------------------------------------- Epoch : 14 -----------------------------------------------------\n",
      "Loss :  52.404922\n",
      "Train Accuracy :  0.965\n",
      "Test Accuracy :  0.8095238\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph_3) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch) :\n",
    "        batch_start=0\n",
    "        total_idx = np.arange(len(train_x)) #; np.random.shuffle(total_idx)\n",
    "        r_max_value = 1\n",
    "        d_max_value = 0\n",
    "        for iteration in range(int(np.ceil(len(train_x)/batch_size))):\n",
    "            batch_idx = total_idx[batch_start:batch_start+batch_size]\n",
    "            batch_x = train_x[total_idx]\n",
    "            batch_y = train_y[total_idx]\n",
    "            if epoch > n_epoch/3:\n",
    "                r_max_value = 1 + 0.1 * epoch\n",
    "                d_max_value = 0.1 * epoch\n",
    "            sess.run(training, feed_dict={x:batch_x,\n",
    "                                          y:batch_y,\n",
    "                                         phase_train:True,\n",
    "                                         r_max : r_max_value,\n",
    "                                         d_max : d_max_value})\n",
    "            batch_start += batch_size\n",
    "        print('------------------------------------------- Epoch : {} -----------------------------------------------------' .format(epoch))\n",
    "        print('Loss : ', sess.run(loss, feed_dict={x:batch_x,\n",
    "                                                   y:batch_y,\n",
    "                                                   phase_train:False,\n",
    "                                                 r_max : r_max_value,\n",
    "                                                 d_max : d_max_value}))\n",
    "        print('Train Accuracy : ', sess.run(acc, feed_dict={x:batch_x,\n",
    "                                                            y:batch_y,\n",
    "                                                            phase_train:False,\n",
    "                                                         r_max : r_max_value,\n",
    "                                                         d_max : d_max_value}))\n",
    "        accuracy = sess.run(acc, feed_dict={x:test_x,\n",
    "                                            y:test_y,\n",
    "                                            phase_train:False,\n",
    "                                             r_max : r_max_value,\n",
    "                                             d_max : d_max_value})\n",
    "        print('Test Accuracy : ', accuracy)\n",
    "        acc_list_3.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between None, Batch Norm, Batch Renorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aa50c05c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mini-batch, non-iid. decay ratio = 0.9\n",
    "plt.plot(acc_list)\n",
    "plt.plot(acc_list_2)\n",
    "plt.plot(acc_list_3)\n",
    "plt.legend(['None', 'Batch Norm', 'Batch Renorm'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
